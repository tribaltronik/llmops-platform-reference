# llmops-platform-reference
Use FastAPI and vLLM for inferencing and deploy it on K8s
